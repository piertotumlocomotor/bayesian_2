{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kedro environment (not needed in .py)\n",
    "from pathlib import Path\n",
    "from kedro.framework.context import load_context\n",
    "\n",
    "# Load a context to be able to work in the notebook\n",
    "#current_dir = Path.cwd()\n",
    "current_dir = Path(\"/u01/share/cesar/aa_engine_uy/notebooks/\")\n",
    "proj_path = current_dir.parent\n",
    "context = load_context(proj_path)\n",
    "catalog = context.catalog\n",
    "credentials = context.config_loader.get(\"credentials*\",\"credentials*/**\")\n",
    "parameters = context.config_loader.get(\"parameters*\",\"parameters*/**\")\n",
    "\n",
    "from aa_engine_pkg.assets.utils import *\n",
    "from aa_engine_pkg.assets.core.data.kedro.catalog_expansion.partitioned_sql import SQLPartitionedDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date='20180605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cliente_activo(cliente_activo: SQLPartitionedDataSet,\n",
    "                          date: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates master table with features related to EoP state of customers for one period of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cliente_activo:\n",
    "        dataset defined in ´catalog.yml´ - list of active customers at EoP for the given period\n",
    "    date:\n",
    "        period to process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Mastertable with information of clientes at EoP\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize logger\n",
    "    log = initialize_logger()\n",
    "\n",
    "    # Load active clientes for period\n",
    "    log.info(f\"Creating cliente_activo...\")\n",
    "    period_to_load = get_previous_month(date)\n",
    "    df_clientes_activos = cliente_activo.filter_by(date=period_to_load)\n",
    "\n",
    "    # Return\n",
    "    return df_clientes_activos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente_activo=catalog.load(\"cliente_activo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente_activo_df= create_cliente_activo(cliente_activo,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'catalog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7a382e32f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcampanas\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"campanas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'catalog' is not defined"
     ]
    }
   ],
   "source": [
    "campanas= catalog.load(\"campanas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vars_to_dummy = [\"OFERTA_COMBINADA\"]\n",
    "\n",
    "past_periods = [14, 21, 28, 84, 168, 252, 336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_campanas(campanas: SQLPartitionedDataSet,\n",
    "                           cliente_activo: pd.DataFrame,\n",
    "                           parameters: Union[Dict, None],\n",
    "                           date: str\n",
    "                           ) -> pd.DataFrame:\n",
    "    \"\"\"Creates master table with offer features for one period of data\n",
    "    Parameters\n",
    "    ----------\n",
    "    campanas:\n",
    "        dataset defined in ``catalog_raw.yml`` with raw data information related to client's offers\n",
    "    cliente_activo:\n",
    "        dataset defined in ``catalog_raw.yml`` with raw data information related to active clients at EoP\n",
    "    date:\n",
    "        period to process\n",
    "    parameters:\n",
    "        set of project parameters defined in ``parameters.yml``\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Master table with offer features for one period\n",
    "    \"\"\"\n",
    "\n",
    "    # Read parameters\n",
    "    log = initialize_logger()\n",
    "\n",
    "    write_to_parquet = parameters[\"write_to_parquet\"]\n",
    "    table_name = \"campanas\"\n",
    "    overwrite = parameters[\"masters\"][table_name][\"overwrite\"]\n",
    "\n",
    "    # Check if table was already created\n",
    "    files = get_mastertable_paths_by_period(parameters=parameters, period=date)\n",
    "    match = [str(file) for file in files if table_name in file]\n",
    "\n",
    "    if len(match) > 0 and overwrite is False:\n",
    "        # If table is found, read parquet:\n",
    "        log.info(f\"Reading {match[0]} table\")\n",
    "        df_final = pd.read_parquet(match[0], engine=\"pyarrow\")\n",
    "\n",
    "    else:\n",
    "        # Read parameters\n",
    "        log = initialize_logger()\n",
    "        look_back_days = parameters[\"masters\"][\"campanas\"][\"look_back_days\"]\n",
    "        start_date = (pd.to_datetime(date) - timedelta(days=look_back_days)).strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Calculate period to load for active clients\n",
    "        period_to_load = get_previous_month(date)\n",
    "        log.info(f\"Loading campanas...\")\n",
    "        df_campanas = campanas.filter_by(date=[start_date, date]).drop_duplicates()\n",
    "\n",
    "        df_clientes = cliente_activo\n",
    "\n",
    "        df_campanas = pd.merge(df_clientes,\n",
    "                               df_campanas,\n",
    "                               on=[\"CUSTOMER_ID\"],\n",
    "                               how=\"inner\")\n",
    "\n",
    "        log.info(\"Creating variables...\")\n",
    "        # Calculate offer duration / days since start / until end\n",
    "        df_campanas['OFFER_DURATION'] = (pd.to_datetime(df_campanas.END_DATE,\n",
    "                                                        errors=\"coerce\") - df_campanas.START_DATE) / np.timedelta64(1,\n",
    "                                                                                                                    \"D\")\n",
    "        df_campanas[\"DAYS_SINCE_START_OFFER\"] = (pd.to_datetime(date) - df_campanas.START_DATE) / np.timedelta64(1, \"D\")\n",
    "        df_campanas[\"DAYS_TO_END_OFFER\"] = (pd.to_datetime(df_campanas.END_DATE, errors=\"coerce\") - pd.to_datetime(\n",
    "            date)) / np.timedelta64(1, \"D\")\n",
    "\n",
    "        # Extract percentages of offers from their description\n",
    "        df_campanas[\"PORC_OFFER\"] = df_campanas[\"DESCRIPTION\"].str.extract(r'(\\d+)%')[0].astype(float)\n",
    "\n",
    "        # Extract discount values of offers from their description\n",
    "        df_campanas[\"VALUE_OFFER\"] = df_campanas[\"DESCRIPTION\"].str.extract(r'\\$(\\d+)')[0].astype(float)\n",
    "\n",
    "        # Make a mask with position of values\n",
    "        maskvalue = df_campanas[\"VALUE_OFFER\"].notna()\n",
    "        maskporc = df_campanas[\"PORC_OFFER\"].notna()\n",
    "\n",
    "        # Create a unique ranking scaled for Percentage and value\n",
    "        maskall = dict(zip([\"VALUE_OFFER\", \"PORC_OFFER\"],\n",
    "                           (maskvalue, maskporc)))\n",
    "        for var, mask in maskall.items():\n",
    "            df_campanas.loc[mask, \"MONTO_OFFER_SCALED\"] = scale_values(df=df_campanas.loc[mask, :],\n",
    "                                                                       vars_to_groupby=[\"DATE_EXP\"],\n",
    "                                                                       var_to_scale=var,\n",
    "                                                                       by_ranking=False)\n",
    "\n",
    "            # Extract length of offer from their description\n",
    "        tuple_len = df_campanas[\"DESCRIPTION\"].str.extract(r'(?:(\\d+)[ ]*M|(\\d+)X)', re.IGNORECASE)[[0, 1]].fillna(\n",
    "            0).astype(int)\n",
    "        df_campanas[\"LENGTH_OFFER\"] = tuple_len.sum(axis=1)\n",
    "\n",
    "        # creo la variable tipo_oferta\n",
    "        condlist = [df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"DSCTO|RET|DESC\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"RENTA\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"UPGRADE\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"NUEVO\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"RECON\")\n",
    "                    ]\n",
    "        choicelist = [\"RETE\",\n",
    "                      \"RENT\",\n",
    "                      \"UPGR\",\n",
    "                      \"NUEV\",\n",
    "                      \"RECO\"]\n",
    "        df_campanas[\"TIPO_OFERTA\"] = np.select(condlist, choicelist, default=\"OTROS\")\n",
    "\n",
    "        # creo variable tipo de producto de la oferta\n",
    "        condlist = [df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\n",
    "            \"PREMIUM|FOX|HBO|EXXXOTICO|HOTPACK|ADULT|CLAXON|HUSTLER\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"PLATA|ORO|BRONCE|SICO\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"BUNDLE\", na=False),\n",
    "                    df_campanas[\"DESCRIPTION\"].str.upper().str.contains(\"NET\", na=False),\n",
    "                    ]\n",
    "        choicelist = [\"PREMIUM\",\n",
    "                      \"BASICO\",\n",
    "                      \"BUNDLE\",\n",
    "                      \"NET\"]\n",
    "        df_campanas[\"PRODUCTO_OFERTA\"] = np.select(condlist, choicelist, default=\"OTROS\")\n",
    "\n",
    "        df_campanas[\"OFERTA_COMBINADA\"] = df_campanas[\"TIPO_OFERTA\"] + \"_\" + df_campanas[\"PRODUCTO_OFERTA\"]\n",
    "        df_campanas[\"FECHA\"] = df_campanas[\"START_DATE\"].dt.strftime(\"%Y%m%d\")\n",
    "\n",
    "        # creo las variables dummy\n",
    "        df_list = []\n",
    "        for var in vars_to_dummy:\n",
    "            log.info(f'---- {var}')\n",
    "            df_list.append(group_categorical_variables(df_campanas,\n",
    "                                                       vars_to_groupby=[\"CUSTOMER_ID\", \"FECHA\"],\n",
    "                                                       var_to_count=var,\n",
    "                                                       other_category=None))\n",
    "\n",
    "        # Reduce list of list to create a data table\n",
    "        df = reduce(lambda left, right: pd.merge(left, right, on=[\"CUSTOMER_ID\", \"FECHA\"], how=\"outer\"), df_list)\n",
    "\n",
    "        ofer_cols = [c for c in df.columns if \"OFERTA\" in c]\n",
    "\n",
    "        df[\"N_OFERTAS\"] = df[ofer_cols].sum(axis=1)\n",
    "\n",
    "        # Join with num vars and expand\n",
    "        df_expanded_offers = add_relative_calculate_past(df=df_campanas[[\"CUSTOMER_ID\",\n",
    "                                                                         \"FECHA\",\n",
    "                                                                         \"DAYS_SINCE_START_OFFER\",\n",
    "                                                                         \"DAYS_TO_END_OFFER\",\n",
    "                                                                         \"LENGTH_OFFER\",\n",
    "                                                                         \"MONTO_OFFER_SCALED\"]],\n",
    "                                                         id_cols=[\"CUSTOMER_ID\"],\n",
    "                                                         periods=past_periods,\n",
    "                                                         agg=[np.nanmean, np.nanmin, np.nanmax],\n",
    "                                                         date_col=\"FECHA\",\n",
    "                                                         start_date=start_date,\n",
    "                                                         end_date=date,\n",
    "                                                         period_freq=\"D\")\n",
    "\n",
    "        # Join with num vars and expand\n",
    "        df_expanded_products = add_relative_calculate_past(df=df,\n",
    "                                                           id_cols=[\"CUSTOMER_ID\"],\n",
    "                                                           periods=past_periods,\n",
    "                                                           agg=[np.nansum],\n",
    "                                                           date_col=\"FECHA\",\n",
    "                                                           start_date=start_date,\n",
    "                                                           end_date=date,\n",
    "                                                           period_freq=\"D\")\n",
    "\n",
    "        # Merge\n",
    "        df_final = df_expanded_products.merge(df_expanded_offers, on=\"CUSTOMER_ID\", validate=\"1:1\")\n",
    "\n",
    "        # Add date variables\n",
    "        df_final[\"DATE_EXP\"] = period_to_load\n",
    "        df_final[\"DATE_CALC\"] = date\n",
    "\n",
    "        # Rename table\n",
    "        rename_table(df=df_final,\n",
    "                     preffix=parameters[\"masters\"][\"campanas\"][\"table_preffix\"],\n",
    "                     ids_to_exclude=[\"CUSTOMER_ID\", \"DATE_EXP\", \"DATE_CALC\"]\n",
    "                     )\n",
    "\n",
    "        # Return\n",
    "        log.info(f\"Exporting {df_final.shape[0]} rows and {df_final.shape[1]} columns\")\n",
    "\n",
    "        if write_to_parquet:\n",
    "            file = f\"{parameters['paths']['master_path']}master_{table_name}/master_{table_name}_{date}.parquet\"\n",
    "            df_final.to_parquet(file, engine=\"pyarrow\")\n",
    "\n",
    "    return df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
