{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kedro environment (not needed in .py)\n",
    "from pathlib import Path\n",
    "from kedro.framework.context import load_context\n",
    "\n",
    "# Load a context to be able to work in the notebook\n",
    "#current_dir = Path.cwd()\n",
    "current_dir = Path(\"/u01/share/cesar/aa_engine_uy/notebooks/\")\n",
    "proj_path = current_dir.parent\n",
    "context = load_context(proj_path)\n",
    "catalog = context.catalog\n",
    "credentials = context.config_loader.get(\"credentials*\",\"credentials*/**\")\n",
    "parameters = context.config_loader.get(\"parameters*\",\"parameters*/**\")\n",
    "\n",
    "from aa_engine_pkg.assets.utils import *\n",
    "from aa_engine_pkg.assets.core.data.kedro.catalog_expansion.partitioned_sql import SQLPartitionedDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date='20180605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cliente_activo(cliente_activo: SQLPartitionedDataSet,\n",
    "                          date: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates master table with features related to EoP state of customers for one period of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cliente_activo:\n",
    "        dataset defined in ´catalog.yml´ - list of active customers at EoP for the given period\n",
    "    date:\n",
    "        period to process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Mastertable with information of clientes at EoP\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize logger\n",
    "    log = initialize_logger()\n",
    "\n",
    "    # Load active clientes for period\n",
    "    log.info(f\"Creating cliente_activo...\")\n",
    "    period_to_load = get_previous_month(date)\n",
    "    df_clientes_activos = cliente_activo.filter_by(date=period_to_load)\n",
    "\n",
    "    # Return\n",
    "    return df_clientes_activos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:21:50,736 - kedro.io.data_catalog - INFO - Loading data from `cliente_activo` (SQLPartitionedDataSet)...\n"
     ]
    }
   ],
   "source": [
    "cliente_activo=catalog.load(\"cliente_activo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:17:54,251 - aa_engine_pkg.assets.utils.utilities - INFO - Creating cliente_activo...\n",
      "select distinct CUSTOMER_ID from stg_uy_customer_status where UPPER(STATUS) LIKE '%ACTIVO%' and DATE_EXP = 201805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u01/miniconda3/envs/uypo37/lib/python3.7/site-packages/sqlalchemy/dialects/oracle/base.py:1381: SAWarning: Oracle version (19, 5, 0, 0, 0) is known to have a maximum identifier length of 128, rather than the historical default of 30. SQLAlchemy 1.4 will use 128 for this database; please set max_identifier_length=128 in create_engine() in order to test the application with this new length, or set to 30 in order to assure that 30 continues to be used.  In particular, pay close attention to the behavior of database migrations as dynamically generated names may change. See the section 'Max Identifier Lengths' in the SQLAlchemy Oracle dialect documentation for background.\n",
      "  % ((self.server_version_info,))\n"
     ]
    }
   ],
   "source": [
    "cliente_activo_df= create_cliente_activo(cliente_activo,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = [\"ARPU\", \"FLAG_DISCOUNT\", \"PROP_DISCOUNT\"]\n",
    "\n",
    "vars_to_group_by = [\"CUSTOMER_ID\"]\n",
    "\n",
    "id_cols = [\"CUSTOMER_ID\", \"DATE_EXP\"]\n",
    "\n",
    "past_periods = [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:18:03,259 - kedro.io.data_catalog - INFO - Loading data from `arpu_quality` (SQLPartitionedDataSet)...\n"
     ]
    }
   ],
   "source": [
    "arpu_quality= catalog.load(\"arpu_quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "log = initialize_logger()\n",
    "\n",
    "write_to_parquet = parameters[\"write_to_parquet\"]\n",
    "table_name = \"arpu_quality\"\n",
    "overwrite = parameters[\"masters\"][table_name][\"overwrite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if table was already created\n",
    "files = get_mastertable_paths_by_period(parameters=parameters, period=date)\n",
    "match = [str(file) for file in files if table_name in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(match) > 0 and overwrite is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:30:34,239 - aa_engine_pkg.assets.utils.utilities - INFO - Loading active customers\n"
     ]
    }
   ],
   "source": [
    "# If not, create table\n",
    "# Calculate period to load for active clients\n",
    "log.info(\"Loading active customers\")\n",
    "df_clientes = cliente_activo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting periods to load\n",
    "look_back_months = parameters[\"masters\"][\"global\"][\"look_back_months\"]\n",
    "periods_to_load = get_last_k_periods(date, look_back_months)\n",
    "start_date = periods_to_load[-1]\n",
    "periods_to_load = tuple(periods_to_load)\n",
    "period_to_load = get_previous_month(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('201806', '201805', '201804', '201803', '201802', '201801')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periods_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:30:38,189 - aa_engine_pkg.assets.utils.utilities - INFO - Loading arpu\n",
      "select * from stg_uy_arpu_quality where CHARGES_YYYYMM in ('201806', '201805', '201804', '201803', '201802', '201801')\n"
     ]
    }
   ],
   "source": [
    "# Get arpu_quality table\n",
    "log.info(\"Loading arpu\")\n",
    "df_aq = arpu_quality.filter_by_period(date=periods_to_load).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:31:04,573 - aa_engine_pkg.assets.utils.utilities - INFO - Merging tables\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Merging tables\")\n",
    "df_aq = pd.merge(df_clientes,\n",
    "                 df_aq,\n",
    "                 on=[\"CUSTOMER_ID\"],\n",
    "                 how=\"inner\",\n",
    "                 validate=\"1:m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:31:14,553 - numexpr.utils - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-12-29 12:31:14,554 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# Fixing variables\n",
    "df_aq[\"DISCOUNT_AMOUNT\"].fillna(0, inplace=True)\n",
    "\n",
    "# Create flag to check for discounts\n",
    "df_aq['FLAG_DISCOUNT'] = np.where(df_aq['DISCOUNT_AMOUNT'] == 0, 0, 1)\n",
    "\n",
    "# Change discount sign to positive\n",
    "df_aq[\"DISCOUNT_AMOUNT\"] = np.abs(df_aq[\"DISCOUNT_AMOUNT\"])\n",
    "\n",
    "# Generate discount ratio between discount and charges\n",
    "create_evolution_variables(df=df_aq,\n",
    "                           var_name='PROP_DISCOUNT',\n",
    "                           denominator='CHARGES_AMOUNT',\n",
    "                           numerator='DISCOUNT_AMOUNT')\n",
    "\n",
    "# Scale value\n",
    "df_aq[\"ARPU\"] = scale_values(df=df_aq,\n",
    "                             vars_to_groupby=[\"DATE_EXP\"],\n",
    "                             var_to_scale=\"ARPU\",\n",
    "                             by_ranking=True)\n",
    "\n",
    "df_aq[\"PROP_DISCOUNT\"] = scale_values(df=df_aq,\n",
    "                                      vars_to_groupby=[\"DATE_EXP\"],\n",
    "                                      var_to_scale=\"PROP_DISCOUNT\",\n",
    "                                      by_ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:31:18,762 - aa_engine_pkg.assets.utils.utilities - INFO - Adding relative date between 201801 and 201805\n",
      "2020-12-29 12:31:19,859 - aa_engine_pkg.assets.utils.utilities - INFO - Expanding table for period: 1\n",
      "2020-12-29 12:31:19,998 - aa_engine_pkg.assets.utils.utilities - INFO - Expanding table for period: 3\n",
      "2020-12-29 12:31:20,272 - aa_engine_pkg.assets.utils.utilities - INFO - Expanding table for period: 6\n"
     ]
    }
   ],
   "source": [
    "# Calculate past variables\n",
    "df_aq_past = add_relative_calculate_past(df_aq,\n",
    "                                         id_cols=[\"CUSTOMER_ID\"],\n",
    "                                         date_col=\"DATE_EXP\",\n",
    "                                         start_date=start_date,\n",
    "                                         end_date=period_to_load,\n",
    "                                         periods=past_periods,\n",
    "                                         period_freq=\"M\",\n",
    "                                         agg={'ARPU': [np.nanmean],\n",
    "                                              'FLAG_DISCOUNT': [np.nansum],\n",
    "                                              'PROP_DISCOUNT': [np.nanmean]},\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:31:24,841 - aa_engine_pkg.assets.utils.utilities - INFO - Calculating ratios\n"
     ]
    }
   ],
   "source": [
    "# Create ratios\n",
    "log.info(\"Calculating ratios\")\n",
    "create_evolution_variables(df=df_aq_past,\n",
    "                           var_name='RATIO_ARPU_1_3',\n",
    "                           numerator='ARPU_nanmean_1',\n",
    "                           denominator='ARPU_nanmean_3')\n",
    "create_evolution_variables(df=df_aq_past,\n",
    "                           var_name='RATIO_ARPU_1_6',\n",
    "                           numerator='ARPU_nanmean_1',\n",
    "                           denominator='ARPU_nanmean_6')\n",
    "\n",
    "# Add date variables\n",
    "df_aq_past[\"DATE_EXP\"] = period_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change variable names \n",
    "table_preffix = parameters[\"masters\"][\"arpu_quality\"][\"table_preffix\"]\n",
    "rename_table(df_aq_past,\n",
    "             preffix=table_preffix,\n",
    "             ids_to_exclude=id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>AQY_ARPU_nanmean_1</th>\n",
       "      <th>AQY_FLAG_DISCOUNT_nansum_1</th>\n",
       "      <th>AQY_PROP_DISCOUNT_nanmean_1</th>\n",
       "      <th>AQY_ARPU_nanmean_3</th>\n",
       "      <th>AQY_FLAG_DISCOUNT_nansum_3</th>\n",
       "      <th>AQY_PROP_DISCOUNT_nanmean_3</th>\n",
       "      <th>AQY_ARPU_nanmean_6</th>\n",
       "      <th>AQY_FLAG_DISCOUNT_nansum_6</th>\n",
       "      <th>AQY_PROP_DISCOUNT_nanmean_6</th>\n",
       "      <th>AQY_RATIO_ARPU_1_3</th>\n",
       "      <th>AQY_RATIO_ARPU_1_6</th>\n",
       "      <th>DATE_EXP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144472</td>\n",
       "      <td>0.953749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038905</td>\n",
       "      <td>0.952094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041554</td>\n",
       "      <td>0.952033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042571</td>\n",
       "      <td>1.001738</td>\n",
       "      <td>1.001802</td>\n",
       "      <td>201805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145360</td>\n",
       "      <td>0.963571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.964279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031307</td>\n",
       "      <td>0.964615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031821</td>\n",
       "      <td>0.999266</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>201805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146099</td>\n",
       "      <td>0.904295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079089</td>\n",
       "      <td>0.902656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082472</td>\n",
       "      <td>0.902659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083819</td>\n",
       "      <td>1.001816</td>\n",
       "      <td>1.001812</td>\n",
       "      <td>201805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.390366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472731</td>\n",
       "      <td>0.384929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>146197</td>\n",
       "      <td>0.979763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017082</td>\n",
       "      <td>0.979985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>0.974408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.999774</td>\n",
       "      <td>1.005495</td>\n",
       "      <td>201805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID  AQY_ARPU_nanmean_1  AQY_FLAG_DISCOUNT_nansum_1  \\\n",
       "0       144472            0.953749                         0.0   \n",
       "1       145360            0.963571                         0.0   \n",
       "2       146099            0.904295                         0.0   \n",
       "3       146123                 NaN                         NaN   \n",
       "4       146197            0.979763                         0.0   \n",
       "\n",
       "   AQY_PROP_DISCOUNT_nanmean_1  AQY_ARPU_nanmean_3  \\\n",
       "0                     0.038905            0.952094   \n",
       "1                     0.030811            0.964279   \n",
       "2                     0.079089            0.902656   \n",
       "3                          NaN            0.390366   \n",
       "4                     0.017082            0.979985   \n",
       "\n",
       "   AQY_FLAG_DISCOUNT_nansum_3  AQY_PROP_DISCOUNT_nanmean_3  \\\n",
       "0                         0.0                     0.041554   \n",
       "1                         0.0                     0.031307   \n",
       "2                         0.0                     0.082472   \n",
       "3                         0.0                     0.472731   \n",
       "4                         0.0                     0.017949   \n",
       "\n",
       "   AQY_ARPU_nanmean_6  AQY_FLAG_DISCOUNT_nansum_6  \\\n",
       "0            0.952033                         0.0   \n",
       "1            0.964615                         0.0   \n",
       "2            0.902659                         0.0   \n",
       "3            0.384929                         0.0   \n",
       "4            0.974408                         0.0   \n",
       "\n",
       "   AQY_PROP_DISCOUNT_nanmean_6  AQY_RATIO_ARPU_1_3  AQY_RATIO_ARPU_1_6  \\\n",
       "0                     0.042571            1.001738            1.001802   \n",
       "1                     0.031821            0.999266            0.998918   \n",
       "2                     0.083819            1.001816            1.001812   \n",
       "3                     0.479281                 NaN                 NaN   \n",
       "4                     0.023502            0.999774            1.005495   \n",
       "\n",
       "  DATE_EXP  \n",
       "0   201805  \n",
       "1   201805  \n",
       "2   201805  \n",
       "3   201805  \n",
       "4   201805  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aq_past.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/uy_po/master/master_arpu_quality/master_arpu_quality_20180605.parquet'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{parameters['paths']['master_path']}master_{table_name}/master_{table_name}_{date}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 12:45:56,310 - aa_engine_pkg.assets.utils.utilities - INFO - Exporting 116786 rows and 13 columns\n"
     ]
    }
   ],
   "source": [
    "if write_to_parquet:\n",
    "            file = f\"{parameters['paths']['master_path']}master_{table_name}/master_{table_name}_{date}.parquet\"\n",
    "            df_aq_past.to_parquet(file, engine=\"pyarrow\")\n",
    "\n",
    "log.info(f\"Exporting {df_aq_past.shape[0]} rows and {df_aq_past.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_arpu_quality(arpu_quality: SQLPartitionedDataSet,\n",
    "                               cliente_activo: pd.DataFrame,\n",
    "                               parameters: Dict,\n",
    "                               date: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates master table with ARPU features for one period of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arpu_quality:\n",
    "        dataset defined in ``catalog.yml`` with raw data information related to ARPU\n",
    "    cliente_activo:\n",
    "        dataset defined in ``catalog.yml`` with raw data information related to active clients at EoP\n",
    "    date:\n",
    "        period to process\n",
    "    parameters:\n",
    "        set of project parameters defined in ``parameters.yml``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Mastertable with ARPU features for one period\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize logger\n",
    "    log = initialize_logger()\n",
    "\n",
    "    write_to_parquet = parameters[\"write_to_parquet\"]\n",
    "    table_name = \"arpu_quality\"\n",
    "    overwrite = parameters[\"masters\"][table_name][\"overwrite\"]\n",
    "\n",
    "    # Check if table was already created\n",
    "    files = get_mastertable_paths_by_period(parameters=parameters, period=date)\n",
    "    match = [str(file) for file in files if table_name in file]\n",
    "\n",
    "    if len(match) > 0 and overwrite is False:\n",
    "        # If table is found, read parquet:\n",
    "        log.info(f\"Reading {match[0]} table\")\n",
    "        df_aq_past = pd.read_parquet(match[0], engine=\"pyarrow\")\n",
    "\n",
    "    else:\n",
    "        # If not, create table\n",
    "        # Calculate period to load for active clients\n",
    "        log.info(\"Loading active customers\")\n",
    "        df_clientes = cliente_activo\n",
    "\n",
    "        # Getting periods to load\n",
    "        look_back_months = parameters[\"masters\"][\"global\"][\"look_back_months\"]\n",
    "        periods_to_load = get_last_k_periods(date, look_back_months)\n",
    "        start_date = periods_to_load[-1]\n",
    "        periods_to_load = tuple(periods_to_load)\n",
    "        period_to_load = get_previous_month(date)\n",
    "\n",
    "        # Get arpu_quality table\n",
    "        log.info(\"Loading arpu\")\n",
    "        df_aq = arpu_quality.filter_by_period(date=periods_to_load).drop_duplicates()\n",
    "\n",
    "        log.info(\"Merging tables\")\n",
    "        df_aq = pd.merge(df_clientes,\n",
    "                         df_aq,\n",
    "                         on=[\"CUSTOMER_ID\"],\n",
    "                         how=\"inner\",\n",
    "                         validate=\"1:m\")\n",
    "\n",
    "        # Fixing variables\n",
    "        df_aq[\"DISCOUNT_AMOUNT\"].fillna(0, inplace=True)\n",
    "\n",
    "        # Create flag to check for discounts\n",
    "        df_aq['FLAG_DISCOUNT'] = np.where(df_aq['DISCOUNT_AMOUNT'] == 0, 0, 1)\n",
    "\n",
    "        # Change discount sign to positive\n",
    "        df_aq[\"DISCOUNT_AMOUNT\"] = np.abs(df_aq[\"DISCOUNT_AMOUNT\"])\n",
    "\n",
    "        # Generate discount ratio between discount and charges\n",
    "        create_evolution_variables(df=df_aq,\n",
    "                                   var_name='PROP_DISCOUNT',\n",
    "                                   denominator='CHARGES_AMOUNT',\n",
    "                                   numerator='DISCOUNT_AMOUNT')\n",
    "\n",
    "        # Scale value\n",
    "        df_aq[\"ARPU\"] = scale_values(df=df_aq,\n",
    "                                     vars_to_groupby=[\"DATE_EXP\"],\n",
    "                                     var_to_scale=\"ARPU\",\n",
    "                                     by_ranking=True)\n",
    "\n",
    "        df_aq[\"PROP_DISCOUNT\"] = scale_values(df=df_aq,\n",
    "                                              vars_to_groupby=[\"DATE_EXP\"],\n",
    "                                              var_to_scale=\"PROP_DISCOUNT\",\n",
    "                                              by_ranking=True)\n",
    "\n",
    "        # Calculate past variables\n",
    "        df_aq_past = add_relative_calculate_past(df_aq,\n",
    "                                                 id_cols=[\"CUSTOMER_ID\"],\n",
    "                                                 date_col=\"DATE_EXP\",\n",
    "                                                 start_date=start_date,\n",
    "                                                 end_date=period_to_load,\n",
    "                                                 periods=past_periods,\n",
    "                                                 period_freq=\"M\",\n",
    "                                                 agg={'ARPU': [np.nanmean],\n",
    "                                                      'FLAG_DISCOUNT': [np.nansum],\n",
    "                                                      'PROP_DISCOUNT': [np.nanmean]},\n",
    "                                                 )\n",
    "        # Create ratios\n",
    "        log.info(\"Calculating ratios\")\n",
    "        create_evolution_variables(df=df_aq_past,\n",
    "                                   var_name='RATIO_ARPU_1_3',\n",
    "                                   numerator='ARPU_nanmean_1',\n",
    "                                   denominator='ARPU_nanmean_3')\n",
    "        create_evolution_variables(df=df_aq_past,\n",
    "                                   var_name='RATIO_ARPU_1_6',\n",
    "                                   numerator='ARPU_nanmean_1',\n",
    "                                   denominator='ARPU_nanmean_6')\n",
    "\n",
    "        # Add date variables\n",
    "        df_aq_past[\"DATE_EXP\"] = period_to_load\n",
    "\n",
    "        # Change variable names \n",
    "        table_preffix = parameters[\"masters\"][\"arpu_quality\"][\"table_preffix\"]\n",
    "        rename_table(df_aq_past,\n",
    "                     preffix=table_preffix,\n",
    "                     ids_to_exclude=id_cols)\n",
    "\n",
    "        if write_to_parquet:\n",
    "            file = f\"{parameters['paths']['master_path']}master_{table_name}/master_{table_name}_{date}.parquet\"\n",
    "            df_aq_past.to_parquet(file, engine=\"pyarrow\")\n",
    "\n",
    "        log.info(f\"Exporting {df_aq_past.shape[0]} rows and {df_aq_past.shape[1]} columns\")\n",
    "\n",
    "    return df_aq_past\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
